{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b28bffbb",
   "metadata": {},
   "source": [
    "## Token classification (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b3f07",
   "metadata": {},
   "source": [
    "The first application we’ll explore is token classification. This generic task encompasses any problem that can be formulated as “attributing a label to each token in a sentence,” such as:\n",
    "\n",
    "Named entity recognition (NER): Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for “no entity.”\n",
    "\n",
    "Part-of-speech tagging (POS): Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n",
    "\n",
    "Chunking: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually B-) to any tokens that are at the beginning of a chunk, another label (usually I-) to tokens that are inside a chunk, and a third label (usually O) to tokens that don’t belong to any chunk.\n",
    "\n",
    "O means the word doesn’t correspond to any entity.\n",
    "B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
    "B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
    "B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
    "B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets tokenizers seqeval evaluate -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003 = datasets.load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c642af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003['train'].features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c887fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003['train'].features['pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003['train'].dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003['train'].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test tokenizer output\n",
    "conll2003['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = conll2003['train'][0]\n",
    "tokenized_input = tokernizer(example_text['tokens'], is_split_into_words=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokernizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a64420",
   "metadata": {},
   "source": [
    "Problem of Sub-Token - The input ids returned by the tokenizer are longer than the lists of labels our dataset contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text['ner_tags'], tokenized_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_text['ner_tags']), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6471a",
   "metadata": {},
   "source": [
    "The below function tokenize_and_align_labels does 2 jobs\n",
    "\n",
    "* set –100 as the label for these special tokens and the subwords we wish to mask during training\n",
    "* mask the subword representations after the first subword <br>\n",
    "\n",
    "Then we align the labels with the token ids using the strategy we picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29de1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True): \n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n",
    "    labels = [] \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]): \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token. \n",
    "        previous_word_idx = None \n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None \n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids: \n",
    "            if word_idx is None:\n",
    "                # set –100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token                 \n",
    "                label_ids.append(label[word_idx]) \n",
    "            else: \n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "                # mask the subword representations after the first subword\n",
    "                 \n",
    "            previous_word_idx = word_idx \n",
    "        labels.append(label_ids) \n",
    "    tokenized_inputs[\"labels\"] = labels \n",
    "    return tokenized_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tokenize_and_align_labels(conll2003['train'][4:5]) \n",
    "print(q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]): \n",
    "    print(f\"{token:_<40} {label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3539d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying on entire data\n",
    "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684708f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8480b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-uncased', num_labels = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ab7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training args\n",
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "args = TrainingArguments( \n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8fa6a",
   "metadata": {},
   "source": [
    "### Lets test the metrix on an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef16f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names \n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003['train'][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label_list[i] for i in conll2003['train'][0][\"ner_tags\"]] \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[labels], references=[labels]) # checking on the training data for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d0de4",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56337d",
   "metadata": {},
   "source": [
    "This compute_metrics() function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don’t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the metric.compute() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds): \n",
    "    pred_logits, labels = eval_preds \n",
    "    \n",
    "    pred_logits = np.argmax(pred_logits, axis=2) \n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "    \n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [ \n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    true_labels = [ \n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "       for prediction, label in zip(pred_logits, labels) \n",
    "   ] \n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return { \n",
    "          \"precision\": results[\"overall_precision\"], \n",
    "          \"recall\": results[\"overall_recall\"], \n",
    "          \"f1\": results[\"overall_f1\"], \n",
    "          \"accuracy\": results[\"overall_accuracy\"], \n",
    "  } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca679701",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer( \n",
    "   model, \n",
    "   args, \n",
    "   train_dataset=tokenized_datasets[\"train\"], \n",
    "   eval_dataset=tokenized_datasets[\"validation\"], \n",
    "   data_collator=data_collator, \n",
    "   tokenizer=tokenizer, \n",
    "   compute_metrics=compute_metrics \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f4f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961afb1c",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4018ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47537f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79176ebb",
   "metadata": {},
   "source": [
    "## Loading model & prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be749848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9669bd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.999496, 'index': 4, 'word': 'hasn', 'start': 11, 'end': 15}, {'entity': 'B-PER', 'score': 0.99936, 'index': 5, 'word': '##ain', 'start': 15, 'end': 18}, {'entity': 'B-LOC', 'score': 0.9995597, 'index': 10, 'word': 'vietnam', 'start': 33, 'end': 40}]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n",
    "\n",
    "example = \"My name is Hasnain and I live in Vietnam\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n",
    "print(len(ner_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e23121",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc308da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated\n"
     ]
    }
   ],
   "source": [
    "if isinstance(ner_results, list) and len(ner_results) == 3:\n",
    "    print(\"Validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3b124",
   "metadata": {},
   "source": [
    "## Upload Artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e982ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_store import CloudSync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c946994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync = CloudSync()\n",
    "sync.upload_ner_config()\n",
    "#sync.download_ner_pytorch_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
